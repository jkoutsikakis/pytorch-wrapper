{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Two Spiral Classification Task <img src=\"https://raw.githubusercontent.com/jkoutsikakis/datasets/master/two_spiral_dataset/two_spiral_dataset.png\" width=\"40px\" height=\"40px\" style='display:inline'/></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will demonstrate how to train and evaluate a model on the TwoSpiral dataset using PyTorchWrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional libraries\n",
    "\n",
    "First of all we need to install the `requests` library in order to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_wrapper as pw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a class that derives from `torch.utils.data.Dataset`. PyTorchWrapper expects that each batch returned by a `torch.utils.data.DataLoader` is represented as a dictionary. This was done in order to be flexible about what information is contained inside a single batch. Since in this case we won't use a custom collate function it is enough to make sure that the `Dataset` object represents a single example as a dictionary. The `DataLoader` will automatically convert a batch of examples (dictionaries) into a single dictionary of examples as follows:\n",
    "\n",
    "[{'input': x1, 'target':y1}, {'input': x2, 'target':y2}] -> DataLoader -> ['input': tensor([x1, x2]), 'target': tensor([y1, y2])]\n",
    "\n",
    "The data will be converted automatically to tensors taking into consideration the type of the original data (`numpy.float32` will become `tensor.float32` is this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoSpiralDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super(TwoSpiralDataset, self).__init__()\n",
    "        \n",
    "        raw_tsv_request = requests.get(\n",
    "            'https://raw.githubusercontent.com/jkoutsikakis/datasets/master/two_spiral_dataset/two_spiral_dataset.tsv'\n",
    "        )\n",
    "\n",
    "        self.pos = []\n",
    "        self.target = []\n",
    "\n",
    "        for line in raw_tsv_request.text.split('\\n')[1:-1]:\n",
    "            pos_x, pos_y, cur_target = line.split('\\t')\n",
    "            self.pos.append([float(pos_x), float(pos_y)])\n",
    "            self.target.append(float(cur_target))\n",
    "\n",
    "        self.pos = np.array(self.pos, dtype='float32')\n",
    "        self.target = np.array(self.target, dtype='float32')\n",
    "\n",
    "    def __getitem__(self, item_index):\n",
    "        return {\n",
    "            'input': self.pos[item_index],\n",
    "            'target': self.target[item_index]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n",
    "Next we define our model. We do so by extending the `toch.nn.Module` class.\n",
    "In this case we will be using a simple MLP with 3 hidden layers of size 128,\n",
    "the ReLU activation function and batch normalization. `pytorch_wrapper.modules.MLP` is one of several ready to use modules provided by PyTorchWrapper.\n",
    "\n",
    "PyTorchWrapper can also handle multi-input models. In such case the dictionary returned by the `torch.utils.data.Dataset`'s `__getitem__`\n",
    "method must contain a list of values at key `'input'` that correspond (one to one) to the arguments of the model's `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.mlp = pw.modules.MLP(\n",
    "            input_size=2,\n",
    "            num_hidden_layers=3,\n",
    "            hidden_layer_size=128,\n",
    "            hidden_activation=nn.ReLU,\n",
    "            hidden_dp=0,\n",
    "            hidden_layer_post_activation_bn=True,\n",
    "            output_size=1,\n",
    "            output_activation=None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Next we create the dataset object along with three data loaders (for training, validation,  and testing). The dataset\n",
    "contains 1000 examples of which 800 will be used for training while the rest subsets will contain 100 examples each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TwoSpiralDataset()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=SubsetRandomSampler(list(range(0, 800))),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=pw.samplers.SubsetSequentialSampler(list(range(800, 900))),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=pw.samplers.SubsetSequentialSampler(list(range(900, 1000))),\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the model and we wrap it with a `pytorch_wrapper.System` object. The `System` object provides methods\n",
    "to train and evaluate the model it contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# last_activation must point to the torch function that needs to be called at non training time.\n",
    "# Some losses (as in this case) work with logits and as such the last activation might not be\n",
    "# performed inside the model's forward method. If the last activation is performed inside the\n",
    "# model then use None.\n",
    "last_activation = torch.nn.Sigmoid()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    system = pw.System(model, last_activation=last_activation, device=torch.device('cuda'))\n",
    "else:\n",
    "    system = pw.System(model, last_activation=last_activation, device=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GenericPointWiseLossWrapper object wraps a native pointwise loss. The batch_target_key\n",
    "# is the key of the dictionary (batch) returned by the DataLoader where it contains the target values.\n",
    "# We specified this key when we defined the dictionary returned by the Dataset's ```__getitem__``` method. For a custom loss\n",
    "# you can implement a class that derives from AbstractLossWrapper. \n",
    "loss_wrapper = pw.loss_wrappers.GenericPointWiseLossWrapper(nn.BCEWithLogitsLoss(),\n",
    "                                                            batch_target_key='target')\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "# Dictionary containing the dataloaders used for evaluation after each epoch.\n",
    "evaluation_data_loaders = {'train': train_data_loader, 'val': val_data_loader}\n",
    "\n",
    "# Dictionary containing the evaluators.\n",
    "evaluators = {'acc': pw.evaluators.AccuracyEvaluator(batch_target_key='target')}\n",
    "\n",
    "# Callback that stops the training process if accuracy does not improve for 20 epochs in the validation set.\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "es_callback = pw.training_callbacks.EarlyStoppingCriterionCallback(\n",
    "    patience=20,\n",
    "    evaluation_data_loader_key='val',\n",
    "    evaluator_key='acc',\n",
    "    tmp_best_state_filepath='tmp/ts_tmp_best.weights'\n",
    ")\n",
    "\n",
    "# The batch_input_key is the key of the dictionary (batch) returned by the dataloader where it contains the\n",
    "# input of the model. We specified this key when we defined the dictionary returned by the Dataset's ```__getitem__``` method.\n",
    "batch_input_key = 'input'\n",
    "\n",
    "_ = system.train(\n",
    "    loss_wrapper=loss_wrapper,\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_data_loader,\n",
    "    evaluators=evaluators,\n",
    "    evaluation_data_loaders=evaluation_data_loaders,\n",
    "    batch_input_key=batch_input_key,\n",
    "    callbacks=[es_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `evaluate` method in order to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = system.evaluate(test_data_loader, evaluators)\n",
    "print(test_results['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `predict` method in order to predict for all the examples returned by a data loder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = system.predict(test_data_loader, perform_last_activation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_pred_pos = 0\n",
    "ex_ds_pos = 900 + ex_pred_pos  # remember we used SubsetSequentialSampler\n",
    "print(f'Prediction for ex {ex_pred_pos}: {predictions[\"outputs\"][ex_pred_pos]}')\n",
    "print(f'Label of ex {ex_pred_pos}: {dataset[ex_ds_pos][\"target\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `predict_batch` method in order to predict for a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.last_activation(system.predict_batch(torch.tensor([[5., 1.]]))).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving & Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save and load the model's weights directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.save_model_state('data/two_spiral_final.weights')\n",
    "_ = system.load_model_state('data/two_spiral_final.weights')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also save and load the whole system at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.save('data/two_spiral_final.system')\n",
    "system = pw.System.load('data/two_spiral_final.system')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
