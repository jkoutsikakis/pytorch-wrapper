{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Task (IMDB Reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will demonstrate how to train and evaluate a Text Classification model on the IMDB Reviews dataset using PyTorchWrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Data\n",
    "First of all we download and extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data /\n",
    "! wget -P data / http: // ai.stanford.edu / ~amaas / data / sentiment / aclImdb_v1.tar.gz\n",
    "! tar xvzf data / aclImdb_v1.tar.gz -C data / > / dev / null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, SubsetRandomSampler\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_wrapper import modules, System\n",
    "from pytorch_wrapper import evaluators as evaluators\n",
    "from pytorch_wrapper.loss_wrappers import GenericPointWiseLossWrapper\n",
    "from pytorch_wrapper.training_callbacks import EarlyStoppingCriterionCallback\n",
    "from pytorch_wrapper.samplers import SubsetSequentialSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Definition\n",
    "In this example we will see how we can use a custom `collate_fn`. This function will be called internally by the\n",
    "dataloaders in order to combine a list of exaples to a ready to use batch. Each individual example will be the output of the `Dataset`'s `__get_item__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    MAX_LEN = 1000\n",
    "\n",
    "    def __init__(self, folder_root, w2i):\n",
    "        self.w2i = w2i\n",
    "        self.ids = []\n",
    "        self.texts = []\n",
    "        self.texts_len = []\n",
    "        self.targets = []\n",
    "\n",
    "        for label in ['pos', 'neg']:\n",
    "            for filepath in tqdm(glob(f'{folder_root}/{label}/*')):\n",
    "                with open(filepath, 'r') as fr:\n",
    "                    ex = fr.read()\n",
    "                _, filename = os.path.split(filepath)\n",
    "                self.ids.append(filename)\n",
    "                text = self.process_example(ex)\n",
    "                self.texts.append(text)\n",
    "                self.texts_len.append(len(text))\n",
    "                self.targets.append(label == 'pos')\n",
    "\n",
    "        self._shuffle_examples()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return (\n",
    "            self.ids[index],\n",
    "            (\n",
    "                self.texts[index],\n",
    "                self.texts_len[index]\n",
    "            ),\n",
    "            self.targets[index]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def _shuffle_examples(self, seed=12345):\n",
    "        \"\"\"\n",
    "        Shuffles the examples with the given seed.\n",
    "        :param seed: The seed used for shuffling.\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        l = list(zip(self.ids, self.texts, self.texts_len, self.targets))\n",
    "        random.shuffle(l)\n",
    "        self.ids, self.texts, self.texts_len, self.targets = zip(*l)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        \"\"\"\n",
    "        Preprocess text.\n",
    "        :param text: Text to be preprocessed.\n",
    "        :return: Preprocessed text.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return ''\n",
    "        text = ''.join([ch if ch.isspace() or ch.isalnum() else '' for ch in text])\n",
    "        text = ' '.join(text.split())\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Function that combines a list of examples into a batch (Called internally by dataloaders).\n",
    "        \"\"\"\n",
    "        batch_zipped = list(zip(*batch))\n",
    "        input_zipped = list(zip(*batch_zipped[1]))\n",
    "\n",
    "        ids = batch_zipped[0]\n",
    "        texts = torch.tensor(IMDBDataset.pad_to_max(input_zipped[0], IMDBDataset.MAX_LEN), dtype=torch.long)\n",
    "        texts_len = torch.tensor(input_zipped[1], dtype=torch.int)\n",
    "        targets = torch.tensor(batch_zipped[2], dtype=torch.float)\n",
    "\n",
    "        return {\n",
    "\n",
    "            'id': ids,\n",
    "            'input': [texts, texts_len],\n",
    "            'target': targets\n",
    "        }\n",
    "\n",
    "    def process_example(self, ex):\n",
    "        \"\"\"\n",
    "        Preprocesses a single example.\n",
    "        :param ex: The text to preprocess.\n",
    "        :return: A list of indexes that correspond to the tokens of the text.\n",
    "        \"\"\"\n",
    "\n",
    "        ex = IMDBDataset.preprocess_text(ex).split()\n",
    "        if len(ex) > 0:\n",
    "            ex = self.convert_tokens_to_indices(ex)\n",
    "        else:\n",
    "            ex = [0]\n",
    "\n",
    "        return ex\n",
    "\n",
    "    def convert_tokens_to_indices(self, token_list, unk_token_index=1):\n",
    "        \"\"\"\n",
    "        Converts Token to indices based on a dictionary.\n",
    "        :param token_list: List of tokens.\n",
    "        :param unk_token_index: Number with which unknown tokens will be replaced.\n",
    "        :return: List of indices.\n",
    "        \"\"\"\n",
    "        return [self.w2i[t] if t in self.w2i else unk_token_index for t in token_list]\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_to_max(lst, max_len=None, pad_int=0):\n",
    "        \"\"\"\n",
    "        Pads the given list of list of tokens to the maximum length.\n",
    "        :param lst: List of list of tokens.\n",
    "        \"\"\"\n",
    "        pad = len(max(lst, key=len))\n",
    "        if max_len is not None:\n",
    "            pad = min(max_len, pad)\n",
    "\n",
    "        return [i + [pad_int] * (pad - len(i)) if len(i) <= pad else i[:pad] for i in lst]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_vocab(folder_root, thr=10):\n",
    "        \"\"\"\n",
    "        Creates a vocabulary from a dataset while discarding words that show up less than 'thr' times.\n",
    "        :param folder_root: The path where the IMDB dataset was extracted.\n",
    "        :param thr: The threshold.\n",
    "        :returns: A list of words and a dictionary that maps the index of a word to the actual word.\n",
    "        \"\"\"\n",
    "        vocab = Counter()\n",
    "        for label in ['pos', 'neg']:\n",
    "            for filepath in tqdm(glob(f'{folder_root}/{label}/*')):\n",
    "                with open(filepath, 'r') as fr:\n",
    "                    ex = fr.read()\n",
    "                text = IMDBDataset.preprocess_text(ex)\n",
    "                vocab.update(text.split())\n",
    "        i2w = ['!!PAD!!', '!!UNK!!'] + [x for x in vocab if vocab[x] >= thr]\n",
    "        w2i = {i2w[i]: i for i in range(len(i2w))}\n",
    "        return w2i, i2w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n",
    "In this example we will use a bidirectional GRU with deep self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Model, self).__init__()\n",
    "        embeddings_size = 128\n",
    "\n",
    "        self.embedding_layer = modules.EmbeddingLayer(\n",
    "            vocab_size,\n",
    "            embeddings_size,\n",
    "            trainable=True,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.text_rnn = nn.GRU(\n",
    "            input_size=embeddings_size,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        text_att_mlp = modules.MLP(\n",
    "            input_size=256,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_layer_size=128,\n",
    "            hidden_activation=nn.ReLU,\n",
    "            output_size=1,\n",
    "            output_activation=None\n",
    "        )\n",
    "\n",
    "        self.text_att = modules.SoftmaxSelfAttentionEncoder(text_att_mlp)\n",
    "\n",
    "        self.output_mlp = modules.MLP(\n",
    "            input_size=256,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_layer_size=128,\n",
    "            hidden_activation=nn.ReLU,\n",
    "            output_size=1,\n",
    "            output_activation=None\n",
    "        )\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "        text = self.embedding_layer(text)\n",
    "        text_rnn_out = self.text_rnn(text)[0]\n",
    "        text_encoding = self.text_att(text_rnn_out, text_len)['output']\n",
    "        out = self.output_mlp(text_encoding).squeeze(-1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Next we create the dataset object along with three data loaders (for training, validation, and testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, i2w = IMDBDataset.create_vocab('data/aclImdb/train/')\n",
    "\n",
    "train_val_dataset = IMDBDataset('data/aclImdb/train/', w2i)\n",
    "test_dataset = IMDBDataset('data/aclImdb/test/', w2i)\n",
    "\n",
    "eval_size = math.floor(0.1 * len(train_val_dataset))\n",
    "train_val_dataset_indexes = list(range(len(train_val_dataset)))\n",
    "train_split_indexes = train_val_dataset_indexes[eval_size:]\n",
    "val_split_indexes = train_val_dataset_indexes[:eval_size]\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(\n",
    "    train_val_dataset,\n",
    "    sampler=SubsetRandomSampler(train_split_indexes),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=IMDBDataset.collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    train_val_dataset,\n",
    "    sampler=SubsetSequentialSampler(val_split_indexes),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=IMDBDataset.collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=IMDBDataset.collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the model and we wrap it with a `System` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(len(i2w))\n",
    "\n",
    "last_activation = nn.Sigmoid()\n",
    "if torch.cuda.is_available():\n",
    "    system = System(model, last_activation=last_activation, device=torch.device('cuda'))\n",
    "else:\n",
    "    system = System(model, last_activation=last_activation, device=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model on the training set, using the validation set for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_wrapper = GenericPointWiseLossWrapper(nn.BCEWithLogitsLoss())\n",
    "evals = {\n",
    "\n",
    "    'acc': evaluators.AccuracyEvaluator(),\n",
    "    'auc': evaluators.AUROCEvaluator()\n",
    "\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, system.model.parameters()))\n",
    "\n",
    "_ = system.train(\n",
    "    loss_wrapper,\n",
    "    optimizer,\n",
    "    train_data_loader=train_dataloader,\n",
    "    evaluators=evals,\n",
    "    evaluation_data_loaders={\n",
    "        'val': val_dataloader\n",
    "    },\n",
    "    callbacks=[\n",
    "        EarlyStoppingCriterionCallback(\n",
    "            patience=3,\n",
    "            evaluation_data_loader_key='val',\n",
    "            evaluator_key='acc',\n",
    "            tmp_best_state_filepath='data/imdb_cur_best.weights'\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = system.evaluate(test_dataloader, evals)\n",
    "for r in results:\n",
    "    print(results[r])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `predict` method in order to predict for all the examples returned by a `Dataloder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = system.predict(test_dataloader, perform_last_activation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 3\n",
    "input_loc = 1\n",
    "text_loc = 0\n",
    "\n",
    "print(' '.join(i2w[x] for x in test_dataset[example_id][input_loc][text_loc]))\n",
    "print(predictions['outputs'][example_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we save the model's state.\n",
    "system.save_model_state('data/imdb_final.weights')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
