{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Task (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will demonstrate how to train and evaluate an Image Classification model on the MNIST dataset using PyTorchWrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional libraries\n",
    "\n",
    "First of all we need to install the `torchvision` library in order to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from pytorch_wrapper import modules, System\n",
    "from pytorch_wrapper import evaluators as evaluators\n",
    "from pytorch_wrapper.loss_wrappers import GenericPointWiseLossWrapper\n",
    "from pytorch_wrapper.training_callbacks import EarlyStoppingCriterionCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Definition\n",
    "Since `torchvision` provides ready to use `Dataset` object for the MNIST Dataset we just need to wrap it with a custom class in order to adhere to the requirements of PyTorchWrapper, i.e. the data loaders must represent a batch as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDatasetWrapper(Dataset):\n",
    "    def __init__(self, is_train):\n",
    "        self.dataset = MNIST(\n",
    "            'data/mnist/',\n",
    "            train=is_train,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.ToTensor()\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'input': self.dataset[index][0], 'target': self.dataset[index][1]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n",
    "We will use a simple 2-layer CNN with an MLP on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, padding=2),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, padding=2),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.out_mlp = modules.MLP(\n",
    "            input_size=980,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_layer_size=128,\n",
    "            hidden_activation=nn.ReLU,\n",
    "            output_size=10,\n",
    "            output_activation=None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.out_mlp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Next we create the dataset object along with three data loaders (for training, validation, and testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = MNISTDatasetWrapper(is_train=True)\n",
    "test_dataset = MNISTDatasetWrapper(is_train=False)\n",
    "\n",
    "# Use 10% of the training dataset as validation.\n",
    "val_size = math.floor(0.1 * len(train_val_dataset))\n",
    "train_val_indexes = list(range(len(train_val_dataset)))\n",
    "random.seed(12345)\n",
    "random.shuffle(train_val_indexes)\n",
    "train_indexes = train_val_indexes[val_size:]\n",
    "val_indexes = train_val_indexes[:val_size]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_val_dataset,\n",
    "    sampler=SubsetRandomSampler(train_indexes),\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    train_val_dataset,\n",
    "    sampler=SubsetRandomSampler(val_indexes),\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the model and we wrap it with a `pytorch_wrapper.System` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "last_activation = nn.Softmax(dim=-1)\n",
    "if torch.cuda.is_available():\n",
    "    system = System(model, last_activation=last_activation, device=torch.device('cuda'))\n",
    "else:\n",
    "    system = System(model, last_activation=last_activation, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model on the training set, using the validation set for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_wrapper = GenericPointWiseLossWrapper(nn.CrossEntropyLoss())\n",
    "evals = {\n",
    "\n",
    "    'prec': evaluators.MultiClassPrecisionEvaluator(average='macro'),\n",
    "    'rec': evaluators.MultiClassRecallEvaluator(average='macro'),\n",
    "    'f1': evaluators.MultiClassF1Evaluator(average='macro')\n",
    "\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, system.model.parameters()))\n",
    "\n",
    "_ = system.train(\n",
    "    loss_wrapper,\n",
    "    optimizer,\n",
    "    train_data_loader=train_dataloader,\n",
    "    evaluators=evals,\n",
    "    evaluation_data_loaders={\n",
    "        'val': val_dataloader\n",
    "    },\n",
    "    callbacks=[\n",
    "        EarlyStoppingCriterionCallback(\n",
    "            patience=3,\n",
    "            evaluation_data_loader_key='val',\n",
    "            evaluator_key='f1',\n",
    "            tmp_best_state_filepath='data/mnist_tmp_best.weights'\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = system.evaluate(test_dataloader, evals)\n",
    "for r in results:\n",
    "    print(results[r])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `predict` method in order to predict for all the examples returned by a dataloder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = system.predict(test_dataloader, perform_last_activation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 599\n",
    "print(f'Prediction for ex {ex}: {np.argmax(predictions[\"outputs\"][ex])}')\n",
    "print(f'Label of ex {ex}: {test_dataset[ex][\"target\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.save_model_state('data/mnist_final.weights')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
