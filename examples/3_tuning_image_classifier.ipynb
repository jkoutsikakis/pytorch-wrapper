{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning an Image Classification model (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will see how we can use PyTorchWrapper in order to tune an Image Classification model on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional libraries\n",
    "\n",
    "First of all we need to install the `torchvision` library in order to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "import random\n",
    "import hyperopt\n",
    "\n",
    "from pprint import pprint\n",
    "from hyperopt import hp\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from pytorch_wrapper import modules, System\n",
    "from pytorch_wrapper import evaluators as evaluators\n",
    "from pytorch_wrapper.loss_wrappers import GenericPointWiseLossWrapper\n",
    "from pytorch_wrapper.training_callbacks import EarlyStoppingCriterionCallback\n",
    "from pytorch_wrapper.tuner import Tuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Definition\n",
    "Since torchvision provides ready to use `torch.utils.data.Dataset` object for the MNIST Dataset we just need to wrap it with a custom class in order to adhere to the requirements of PyTorchWrapper, i.e. the data loaders must represent a batch as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDatasetWrapper(Dataset):\n",
    "    def __init__(self, is_train):\n",
    "        self.dataset = MNIST(\n",
    "            'data/mnist/',\n",
    "            train=is_train,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.ToTensor()\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'input': self.dataset[index][0], 'target': self.dataset[index][1]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n",
    "The model will be CNN based, but the exact architecture will be chosen by the tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, depth, dp, mlp_depth, mlp_hl):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        cnn_list = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=math.floor(kernel_size / 2)\n",
    "            ),\n",
    "            nn.Dropout(p=dp),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            cnn_list.extend([\n",
    "                nn.Conv2d(\n",
    "                    in_channels=channels,\n",
    "                    out_channels=channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=math.floor(kernel_size / 2)\n",
    "                ),\n",
    "                nn.Dropout(p=dp),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "\n",
    "        self.cnn = nn.Sequential(*cnn_list)\n",
    "        self.out_mlp = modules.MLP(\n",
    "            input_size=int(pow(int(28 // (math.pow(2, depth))), 2)) * channels,\n",
    "            num_hidden_layers=mlp_depth,\n",
    "            hidden_layer_size=mlp_hl,\n",
    "            hidden_activation=nn.ReLU,\n",
    "            hidden_dp=dp,\n",
    "            output_size=10,\n",
    "            output_activation=None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.out_mlp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "First of all we create the dataset objects alongside four data loaders. The train_dataloader will be used for training, the val_dataloader for early stopping, the dev_dataloader for hyperparameter optimization, and the test_dataloader\n",
    "for the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dev_dataset = MNISTDatasetWrapper(True)\n",
    "test_dataset = MNISTDatasetWrapper(False)\n",
    "\n",
    "eval_size = math.floor(0.1 * len(train_val_dev_dataset))\n",
    "train_val_dev_indexes = list(range(len(train_val_dev_dataset)))\n",
    "random.seed(12345)\n",
    "random.shuffle(train_val_dev_indexes)\n",
    "train_indexes = train_val_dev_indexes[eval_size * 2:]\n",
    "val_indexes = train_val_dev_indexes[eval_size:eval_size * 2]\n",
    "dev_indexes = train_val_dev_indexes[:eval_size]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_val_dev_dataset,\n",
    "    sampler=SubsetRandomSampler(train_indexes),\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    train_val_dev_dataset,\n",
    "    sampler=SubsetRandomSampler(val_indexes),\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    train_val_dev_dataset,\n",
    "    sampler=SubsetRandomSampler(dev_indexes),\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the step function. This function is called in the beginning of each iteration of the tuning process.\n",
    "This function is responsible for creating, training and evaluating the model given the chosen hyper parameters. The goal of the tuning process is to find the hyper parameters that minimize a chosen metric. In this example we try to minimize\n",
    "the **negative** f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(current_params):\n",
    "    model = Model(**current_params['model_params'])\n",
    "\n",
    "    last_activation = nn.Softmax(dim=-1)\n",
    "    if torch.cuda.is_available():\n",
    "        system = System(model, last_activation=last_activation, device=torch.device('cuda'))\n",
    "    else:\n",
    "        system = System(model, last_activation=last_activation, device=torch.device('cpu'))\n",
    "\n",
    "    loss_wrapper = GenericPointWiseLossWrapper(nn.CrossEntropyLoss())\n",
    "    evals = {\n",
    "\n",
    "        'prec': evaluators.MultiClassPrecisionEvaluator(average='macro'),\n",
    "        'rec': evaluators.MultiClassRecallEvaluator(average='macro'),\n",
    "        'f1': evaluators.MultiClassF1Evaluator(average='macro')\n",
    "\n",
    "    }\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, system.model.parameters()),\n",
    "                                 lr=current_params['training_params']['lr'])\n",
    "\n",
    "    _ = system.train(\n",
    "        loss_wrapper,\n",
    "        optimizer,\n",
    "        train_data_loader=train_dataloader,\n",
    "        evaluators=evals,\n",
    "        evaluation_data_loaders={\n",
    "            'val': val_dataloader\n",
    "        },\n",
    "        callbacks=[\n",
    "            EarlyStoppingCriterionCallback(\n",
    "                3,\n",
    "                'val',\n",
    "                'f1',\n",
    "                'data/mnist_tuning_cur_best.weights'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return -system.evaluate(dev_dataloader, evals)['f1'].score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the hyper_parameter_generators, create the tuner and run it. For more information about the definition of the hyper_parameter_generators check the HyperOpt documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameter_generators = {\n",
    "\n",
    "    'model_params': {\n",
    "        'channels': hp.choice('channels', [5, 10, 20, 30, 50]),\n",
    "        'kernel_size': hp.choice('kernel_size', [3, 5, 7]),\n",
    "        'depth': hp.choice('depth', [1, 2, 3, 4]),\n",
    "        'dp': hp.uniform('dp', 0, 0.5),\n",
    "        'mlp_depth': hp.choice('mlp_depth', [1, 2, 3, 4]),\n",
    "        'mlp_hl': hp.choice('mlp_hl', [32, 64, 128, 256])\n",
    "    },\n",
    "\n",
    "    'training_params': {\n",
    "        'lr': hp.loguniform('lr', math.log(0.0001), math.log(0.1))\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "tuner = Tuner(\n",
    "    hyper_parameter_generators,\n",
    "    step_function=step_function,\n",
    "    algorithm=hyperopt.tpe.suggest,\n",
    "    fit_iterations=20\n",
    ")\n",
    "\n",
    "results = tuner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
