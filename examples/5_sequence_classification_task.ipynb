{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification Task (POS Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will see how we can use PyTorchWrapper to tackle the task of pos tagging in the Penn Treebank\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional libraries\n",
    "\n",
    "First of all we need to install the `nltk` library in order to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Data\n",
    "Next we download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.corpus import treebank\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from pytorch_wrapper import modules, System\n",
    "from pytorch_wrapper import evaluators as evaluators\n",
    "from pytorch_wrapper.samplers import SubsetSequentialSampler\n",
    "from pytorch_wrapper.loss_wrappers import SequenceLabelingGenericPointWiseLossWrapper\n",
    "from pytorch_wrapper.training_callbacks import EarlyStoppingCriterionCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeBankDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, w2i, l2i):\n",
    "        \n",
    "        self.ids = []\n",
    "        self.texts = []\n",
    "        self.texts_len = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for i, ex in enumerate(tqdm(sentences)):\n",
    "            self.ids.append(i)\n",
    "            tokens, labels = list(zip(*ex))\n",
    "            self.texts.append(TreeBankDataset.convert_tokens_to_indices(tokens, w2i))\n",
    "            self.texts_len.append(len(tokens))\n",
    "            self.targets.append(TreeBankDataset.convert_tokens_to_indices(labels, l2i))\n",
    "\n",
    "        self._shuffle_examples()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return (self.ids[index],\n",
    "                (self.texts[index],\n",
    "                 self.texts_len[index]),\n",
    "                self.targets[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "                \n",
    "    def _shuffle_examples(self, seed=12345):\n",
    "        \"\"\"\n",
    "        Shuffles the examples with the given seed.\n",
    "        :param seed: The seed used for shuffling.\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        l = list(zip(self.ids, self.texts, self.texts_len, self.targets))\n",
    "        random.shuffle(l)\n",
    "        self.ids, self.texts, self.texts_len, self.targets = zip(*l)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Function that combines a list of examples in order to a batch. Called internally\n",
    "        by dataloaders.\n",
    "        \"\"\"\n",
    "        batch_zipped = list(zip(*batch))\n",
    "        input_zipped = list(zip(*batch_zipped[1]))\n",
    "        ids = batch_zipped[0]\n",
    "        texts = torch.tensor(TreeBankDataset.pad_to_max(input_zipped[0]), dtype=torch.long)\n",
    "        texts_len = torch.tensor(input_zipped[1], dtype=torch.int)\n",
    "        targets = torch.tensor(TreeBankDataset.pad_to_max(batch_zipped[2]), dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'id': ids,\n",
    "            'input': [texts, texts_len],\n",
    "            'target': targets\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_tokens_to_indices(token_list, t2i, unk_token_index=1):\n",
    "        return [t2i[t] if t in t2i else unk_token_index for t in token_list]\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_to_max(lst, pad_int=0):\n",
    "        pad = len(max(lst, key=len))\n",
    "        return [i + [pad_int] * (pad - len(i)) if len(i) <= pad else i[:pad] for i in lst]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_vocab(sentences):\n",
    "        vocab = set()\n",
    "        labels = set()\n",
    "        for s in tqdm(sentences):\n",
    "            s_tokens, s_labels = list(zip(*s))\n",
    "            vocab.update(s_tokens)\n",
    "            labels.update(s_labels)\n",
    "        i2w = ['!!PAD!!', '!!UNK!!'] + [x for x in vocab]\n",
    "        w2i = {i2w[i]:i for i in range(len(i2w))}\n",
    "        \n",
    "        i2l = [x for x in s_labels]\n",
    "        l2i = {i2l[i]:i for i in range(len(i2l))}\n",
    "        \n",
    "        return w2i, i2w, l2i, i2l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n",
    "In this example we will use a bidirectional GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, output_size):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        embeddings_size = 128\n",
    "\n",
    "        self.embedding_layer = modules.EmbeddingLayer(vocab_size,\n",
    "                                                      embeddings_size,\n",
    "                                                      trainable=True,\n",
    "                                                      padding_idx=0)\n",
    "\n",
    "        self.text_rnn = nn.GRU(input_size=embeddings_size,\n",
    "                               hidden_size=128,\n",
    "                               num_layers=2,\n",
    "                               bidirectional=True,\n",
    "                               batch_first=True)\n",
    "\n",
    "        self.output_mlp = modules.MLP(\n",
    "            input_size=256,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_layer_size=128,\n",
    "            hidden_activation=nn.ReLU,\n",
    "            output_size=output_size,\n",
    "            output_activation=None\n",
    "        )\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "        \n",
    "        text = self.embedding_layer(text)\n",
    "        text_rnn_out = self.text_rnn(text)[0]\n",
    "        out = self.output_mlp(text_rnn_out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Next we create the dataset object along with three data loaders (for training, validation, and testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "w2i, i2w, l2i, i2l = TreeBankDataset.create_vocab(sentences)\n",
    "\n",
    "dataset = TreeBankDataset(sentences, w2i, l2i)\n",
    "\n",
    "eval_size = math.floor(0.1 * len(dataset))\n",
    "dataset_indicis = list(range(len(dataset)))\n",
    "train_split_indicis = dataset_indicis[2*eval_size:]\n",
    "val_split_indicis = dataset_indicis[eval_size:2*eval_size]\n",
    "test_split_indicis = dataset_indicis[:eval_size]\n",
    "\n",
    "train_dataloader = DataLoader(dataset, \n",
    "                              sampler=SubsetRandomSampler(train_split_indicis), \n",
    "                              batch_size=128,\n",
    "                              collate_fn=TreeBankDataset.collate_fn)\n",
    "\n",
    "val_dataloader = DataLoader(dataset, \n",
    "                            sampler=SubsetSequentialSampler(val_split_indicis), \n",
    "                            batch_size=128,\n",
    "                            collate_fn=TreeBankDataset.collate_fn)\n",
    "\n",
    "test_dataloader = DataLoader(dataset, \n",
    "                             sampler=SubsetSequentialSampler(test_split_indicis), \n",
    "                             batch_size=128,\n",
    "                             collate_fn=TreeBankDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the model and we wrap it with a System object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(len(i2w), len(i2l))\n",
    "\n",
    "last_activation = nn.Softmax(dim=-1)\n",
    "if torch.cuda.is_available():\n",
    "    system = System(model, last_activation=last_activation, device=torch.device('cuda'))\n",
    "else:\n",
    "    system = System(model, last_activation=last_activation, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model on the training set, using the validation set for early stopping. PyTorchWrapper provides\n",
    "`pytorch_wrapper.loss_wrappers.SequenceLabelingGenericPointWiseLossWrapper` that wraps a native pointwise loss and `pytorch_wrapper.evaluators.SequenceLabelingEvaluatorWrapper` which wraps an evaluator. These two classes make sure that labels\n",
    "that correspond to padding tokens are ignored. For this reason they need the `batch_input_sequence_length_idx` \n",
    "argument that points to the position of the input list where the length of each example of the batch resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_wrapper = SequenceLabelingGenericPointWiseLossWrapper(loss=nn.CrossEntropyLoss(),\n",
    "                                                           batch_input_sequence_length_idx=1)\n",
    "    \n",
    "evals = {\n",
    "\n",
    "    'prec': evaluators.SequenceLabelingEvaluatorWrapper(\n",
    "        evaluators.MultiClassPrecisionEvaluator(average='macro'), batch_input_sequence_length_idx=1),\n",
    "    \n",
    "    'rec': evaluators.SequenceLabelingEvaluatorWrapper(\n",
    "        evaluators.MultiClassRecallEvaluator(average='macro'), batch_input_sequence_length_idx=1),\n",
    "    \n",
    "    'f1': evaluators.SequenceLabelingEvaluatorWrapper(\n",
    "        evaluators.MultiClassF1Evaluator(average='macro'), batch_input_sequence_length_idx=1)\n",
    "\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, system.model.parameters()))\n",
    "\n",
    "_ = system.train(loss_wrapper,\n",
    "                 optimizer,\n",
    "                 train_data_loader=train_dataloader,\n",
    "                 evaluators=evals,\n",
    "                 evaluation_data_loaders={\n",
    "                    'val': val_dataloader\n",
    "                 },\n",
    "                 callbacks=[\n",
    "                     EarlyStoppingCriterionCallback(patience=3, \n",
    "                                                    evaluation_data_loader_key='val', \n",
    "                                                    evaluator_key='f1', \n",
    "                                                    tmp_best_state_filepath='data/pos_tagging_cur_best.weights')\n",
    "                 ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = system.evaluate(test_dataloader, evals)\n",
    "for r in results:\n",
    "    print(results[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `predict` method in order to predict for all the examples returned by a `Dataloder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = system.predict(test_dataloader, perform_last_activation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 50\n",
    "input_loc = 1\n",
    "text_loc = 0 \n",
    "\n",
    "tokens = [i2w[x] for x in dataset[test_split_indicis[example_id]][input_loc][text_loc]]\n",
    "predicted_labes = [i2l[np.argmax(scores)] for scores in predictions['outputs'][example_id][:len(tokens)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(tokens, predicted_labes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we save the model's state.\n",
    "system.save_model_state('data/pos_tagging_final.weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
